<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Codes on Cindy&#39;s Blog</title>
    <link>http://xmxiaohuilin.github.io/code/</link>
    <description>Recent content in Codes on Cindy&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>(c) 2015 Cindy Lin.</copyright>
    <lastBuildDate>Tue, 24 Nov 2015 22:56:15 -0700</lastBuildDate>
    <atom:link href="http://xmxiaohuilin.github.io/code/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Apache and PHP Configuration and some shell scripts</title>
      <link>http://xmxiaohuilin.github.io/code/Apache/</link>
      <pubDate>Tue, 24 Nov 2015 22:56:15 -0700</pubDate>
      
      <guid>http://xmxiaohuilin.github.io/code/Apache/</guid>
      <description>

&lt;p&gt;This document is for the technical operations assignment. The assignment mainly target to achieve the apache httpd and PHP configuration on AWS. There are some bash scripts practices also required in this assignment.&lt;/p&gt;

&lt;h2 id=&#34;task-1&#34;&gt;Task 1:&lt;/h2&gt;

&lt;h3 id=&#34;install-apache-server-httpd-and-open-ssl&#34;&gt;Install Apache Server (httpd) and Open SSL:&lt;/h3&gt;

&lt;p&gt;Access AWS:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh root@ec2-54-145-34-67.compute-1.amazonaws.com 
# Enter your password
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Download Apache HTTPD&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd /home/root/

wget http://apache.osuosl.org/httpd/httpd-2.2.31.tar.gz

mkdir apache-2.2

mv httpd-2.2.31.tar.gz apache-2.2

cd apache-2.2

tar -xzvf httpd-2.2.31.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Download and install open ssl so that apache can achieve the https visiting&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget ftp://ftp.openssl.org/source/openssl-0.9.8zd.tar.gz

tar -zxvf openssl-0.9.8zd.tar.gz

# Please config with -fPIC otherwise it may takes error during httpd
./config -fPIC no-gost no-shared no-zlib

make
make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;configuration-on-both-httpd-and-open-ssl-certificates&#34;&gt;Configuration on both HTTPD and Open SSL Certificates&lt;/h3&gt;

&lt;p&gt;Configure and install the HTTPD&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd /home/root/apache-2.2/httpd-2.2.31

# Configure Apache to be installed into your home directory
# Sometime it may show some errors like: `no acceptable C compiler found in $Path`
# Please check gcc package installed! 
# make agrs: --enable-so --enable-modules=all --enable-mods-shared=all so that it can dynamic load modules for SSL and php
./configure --prefix=/home/root/apache-2.2 --enable-so --enable-modules=all --enable-mods-shared=all --enable-ssl=shared --with-ssl=/usr/local/ssl 

make
make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Generate Open SSL Key and Certificates&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# enter the openssl directory
cd /usr/local/ssl/bin

#Generate private key   
openssl genrsa -out server.key 2048   

#Generate CSR Certificate Signing Request   
openssl req -new -key server.key -out server.csr  

#Generate Self Signed Certificate  
openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt  

# Copy all generated certificates and keys into directory under apache
cp server.key   /home/root/apache-2.2/conf/
cp server.csr   /home/root/apache-2.2/conf/
cp server.crt   /home/root/apache-2.2/conf/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Configure the ssl.conf for httpd&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;nano /home/root/apache-2.2/conf/extra/httpd-ssl.conf

# Modify following 
SSLCertificateFile conf/server.key            # configure the certificate of server
SSLCertificateKeyFile conf/server.csr        # configure the private key 
#SSLCertificateChainFile conf/server.crt     # remove&#39;#&#39;

# Configure the httpd.conf - the global httpd configuration file
nano /home/root/apache-2.2/conf/httpd.conf

# Configure Apache to listen on both port 81 AND 82 simultaneously
# Listen 81 # remove&#39;#&#39;
# Listen 82 # remove&#39;#&#39;
# Include conf/extra/httpd-ssl.conf #remove&#39;#&#39;

# Check the configuration and run the apache server
cd /home/root/apache-2.2/bin
./apachectl configtest
./apachectl -k start

# Verify that Apache is listening on both port 81 AND 82 AND HTTPS/443
curl http://localhost:81
# It will show this:
&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;It works!&amp;lt;/h1&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
curl http://localhost:82
curl --cacert &amp;quot;/usr/local/ssl/bin/server.crt&amp;quot; https://localhost
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;task-2&#34;&gt;Task 2:&lt;/h2&gt;

&lt;h3 id=&#34;get-php-installed&#34;&gt;Get PHP installed&lt;/h3&gt;

&lt;p&gt;The prerequisite (libxml2) for php install&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mkdir php
cd php

wget http://xmlsoft.org/sources/libxml2-2.9.1.tar.gz
tar zxvf libxml2-2.9.1.tar.gz 
 cd libxml2-2.9.1.
 # --with-python=no for sometime the error occured when install
  ./configure --prefix=/usr/local/libxml2 --with-python=no
 make
 # make clean  # when some error occur
 make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Install and Configuration on PHP&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget  http://us2.php.net/get/php-5.4.45.tar.gz/from/this/mirror
tar zxvf php-5.4.45
cd php-5.4.45

# Configure php install with pointing the root path of httpd and php and also the libxml2
./configure --with-apxs2=/home/root/apache-2.2/bin/apxs --prefix=/home/root/apache-2.2/php --with-config-file-path=/home/root/apache-2.2/php --with-libxml-dir=/usr/local/libxml2

# Move php.ini into the php path
cd /home/root/php/php-5.4.45
mv php.ini-development php.ini.dist
cp php.ini.dist /home/root/apache-2.2/php/bin/php.ini 


# Add the following two lines in httpd.conf
AddType application/x-httpd-php .php 
AddType application/x-httpd-php-source .phps   

# Make sure you have this line in httpd.conf
LoadModule php5_module        modules/libphp5.so

cd /home/root/apache-2.2/bin
./apachectl stop
./apachectl start
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;write-and-test-following-into-helloworld-php&#34;&gt;Write and Test following into &amp;lsquo;helloworld.php&amp;rsquo;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;nano /home/root/apache-2.2/htdocs/helloworld.php

# Write down following into helloworld.php
&amp;lt;html&amp;gt;
 &amp;lt;head&amp;gt;
  &amp;lt;title&amp;gt;PHP Test&amp;lt;/title&amp;gt;
 &amp;lt;/head&amp;gt;
 &amp;lt;body&amp;gt;
 &amp;lt;?php echo &#39;&amp;lt;p&amp;gt;Hello World&amp;lt;/p&amp;gt;&#39;; ?&amp;gt; 
 &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;

# Test
curl localhost:81/helloworld.php

#Output
&amp;lt;html&amp;gt;
 &amp;lt;head&amp;gt;
  &amp;lt;title&amp;gt;PHP Test&amp;lt;/title&amp;gt;
 &amp;lt;/head&amp;gt;
 &amp;lt;body&amp;gt;
 &amp;lt;p&amp;gt;Hello World&amp;lt;/p&amp;gt; 
 &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;task-3&#34;&gt;Task 3:&lt;/h2&gt;

&lt;h3 id=&#34;task-3a-prompt-for-your-name-and-print-it-out-by-write-following-script&#34;&gt;Task 3A: Prompt for your name and print it out by write following script:&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;name=&#39;Rui Bi&#39;
echo $name
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;task-3b-print-out-the-current-date-five-times-with-a-minute-pause-between-each-print&#34;&gt;Task 3B: Print out the current date five times, with a minute pause between each print.&lt;/h3&gt;

&lt;p&gt;By write following script:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;for ((i=0;$i&amp;lt;5;i++))
 do
 date
 sleep 60
 done
echo
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;task-3c-count-the-number-of-processes-running-as-root&#34;&gt;Task 3C: Count the number of processes running as &amp;ldquo;&amp;ldquo;root&amp;rdquo;&amp;rdquo;&lt;/h3&gt;

&lt;p&gt;By write following script:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;count=`ps -u root | wc  -l`
echo $count
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;task-3d-every-minute-you-should-print-out-the-following&#34;&gt;Task 3D: Every minute, you should print out the following&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;The total number of processes running as root.  If this is above 25, print out a warning message.&lt;/li&gt;
&lt;li&gt;The percentage of disk used.  If this is above 80%, print out a warning message.&lt;/li&gt;
&lt;li&gt;The amount of free memory, if this number gets below 100mb, print out a warning message.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;write-a-shell-script-systemmonitor-sh-which-is-storing-in-directory-home-root-as-following&#34;&gt;Write a shell script &lt;code&gt;SystemMonitor.sh&lt;/code&gt; which is storing in directory &lt;code&gt;/home/root/&lt;/code&gt;, as following:&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;psCnt=`ps -u root | wc  -l`
diskUsg=`df / | awk &#39;END{print $5}&#39; | cut -f 1 -d &amp;quot;%&amp;quot;`
freeMem=`free -m | grep Mem | awk &#39;{print $4}&#39;`
 
if (($psCnt &amp;gt; 25)); then
 echo &amp;quot;Warning: current process amount is &amp;quot;$psCnt
fi

if ((diskUsg &amp;gt; 80)); then
 echo &amp;quot;Warning: current disk Usage exceeded 80%&amp;quot;
fi

if (($freeMem &amp;lt; 100)); then
 echo &amp;quot;Warning: current free Memory less than 100MB&amp;quot;
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;setup-crontab-by-following-command-and-output-the-result-in-home-root-monitor-output-file&#34;&gt;Setup &lt;code&gt;crontab&lt;/code&gt; by following command and output the result in &lt;code&gt;/home/root/monitor_output.file&lt;/code&gt;&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;crontab -e
*/1 * * * * /home/root/SystemMonitor.sh &amp;gt;&amp;gt; /home/root/monitor_output.file &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;task-3e-create-a-crontab-that-will-execute-every-5-minutes-that-will-log-the-last-4-ip-addresses-that-accessed-your-website&#34;&gt;Task 3E: Create a crontab that will execute every 5 minutes that will log the last 4 IP addresses that accessed your website.&lt;/h3&gt;

&lt;h4 id=&#34;for-this-task-we-have-to-check-the-httpd-access-log-which-is-located-in-home-root-apache-2-2-logs-access-log&#34;&gt;For this task, we have to check the httpd access log which is located in &lt;code&gt;/home/root/apache-2.2/logs/access-log&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;Here I use &lt;code&gt;cat&lt;/code&gt; command and output the result to the &lt;code&gt;/home/root/ip_output.file&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&#34;write-a-shell-script-ip-monitor-sh&#34;&gt;Write a shell script &lt;code&gt;ip_monitor.sh&lt;/code&gt;&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;echo `date` &amp;gt;&amp;gt; /home/root/ip_output.file

# Get recent 4 ip access means the last 4 ip in access_log
cat /home/root/apache-2.2/logs/access_log |cut -d &#39; &#39; -f 1 | awk &#39;{print $0 }&#39; | tail -n 4 |less &amp;gt;&amp;gt; /home/root/ip_output.file

# for first 5 frequent visit use this:
# /home/root/apache-2.2/logs/access_log |cut -d &#39; &#39; -f 1 |sort |uniq -c | sort -nr | awk &#39;{print $0 }&#39; | head -n 5 |less
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;make-the-ip-access-log-monitor-as-a-crontab-task-with-every-5-minutes-run&#34;&gt;Make the ip access log monitor as a &lt;code&gt;crontab&lt;/code&gt; task with every 5 minutes run.&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;crontab -e
*/5 * * * * /home/root/ip_monitor.sh
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Hadoop Review</title>
      <link>http://xmxiaohuilin.github.io/code/reviewHadoop/</link>
      <pubDate>Thu, 12 Nov 2015 22:56:15 -0700</pubDate>
      
      <guid>http://xmxiaohuilin.github.io/code/reviewHadoop/</guid>
      <description>

&lt;h2 id=&#34;what-is-hadoop&#34;&gt;What is Hadoop&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;An open-source software framework for storage and large-scale processing of data-sets on clusters of commodity hardware&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmruibi/Sketchboard/master/hadoopInfra.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;physical-hardware-level&#34;&gt;Physical Hardware Level&lt;/h2&gt;

&lt;p&gt;This is the hardware part of the infrastructure.&lt;/p&gt;

&lt;h3 id=&#34;cluster&#34;&gt;Cluster&lt;/h3&gt;

&lt;p&gt;Cluster is the set of host machines (nodes). Nodes may be partitioned in racks.&lt;/p&gt;

&lt;h2 id=&#34;hardware-management-level&#34;&gt;Hardware Management Level&lt;/h2&gt;

&lt;p&gt;This level contains the functions for manipulate the physical resources, management on data resources and static storage for data resources.&lt;/p&gt;

&lt;h3 id=&#34;yarn&#34;&gt;YARN&lt;/h3&gt;

&lt;p&gt;YARN Infrastructure (Yet Another Resource Negotiator) is to do the Computational Resources Allocation. We can also regard it as a data operating system.&lt;/p&gt;

&lt;h5 id=&#34;resource-manager-one-per-cluster-is-the-master&#34;&gt;&lt;strong&gt;Resource Manager&lt;/strong&gt; (one per cluster) is the master.&lt;/h5&gt;

&lt;p&gt;Knows how many resources they have.
- Scheduler
- Heartbeat Monitor&lt;/p&gt;

&lt;h5 id=&#34;node-manager-many-per-cluster-is-the-slave-of-the-infrastructure&#34;&gt;&lt;strong&gt;Node Manager&lt;/strong&gt; (many per cluster) is the slave of the infrastructure.&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Heartbeat Sender&lt;/em&gt;&lt;br /&gt;
Periodically, it sends an heartbeat to the Resource Manager.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Container Management&lt;/em&gt;&lt;br /&gt;
A fraction of the NM capacity and it is used by the client for running a program.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;hdfs&#34;&gt;HDFS&lt;/h3&gt;

&lt;p&gt;File System for providing permanent, reliable and distributed storage. This is typically used for storing inputs and output (but not intermediate ones).&lt;/p&gt;

&lt;h3 id=&#34;storage&#34;&gt;Storage&lt;/h3&gt;

&lt;p&gt;Alternative storage solutions. For instance, Database (like &lt;strong&gt;MongoDB&lt;/strong&gt;) or Simple Storage Service (S3).&lt;/p&gt;

&lt;h2 id=&#34;software-level-map-reduce&#34;&gt;Software Level (Map Reduce)&lt;/h2&gt;

&lt;p&gt;This level mainly focus on the data processing by implementing the &lt;strong&gt;MapReduce&lt;/strong&gt; paradigm.&lt;/p&gt;

&lt;h3 id=&#34;before-mapreduce-job&#34;&gt;Before MapReduce Job&lt;/h3&gt;

&lt;p&gt;When a client submits an application, several kinds of information are provided to the YARN infrastucture. In particular:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Configuration:&lt;br /&gt;
This may be partial (some parameters are not specified by the user) and in this case the default values are used for the job. Notice that these default values may be the ones chosen by a Hadoop provider like Amanzon.&lt;/li&gt;
&lt;li&gt;Java Implementation:

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;map()&lt;/code&gt; implementation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;combiner()&lt;/code&gt; implementation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;reduce()&lt;/code&gt; implementation&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Input/Output information:

&lt;ul&gt;
&lt;li&gt;Input URL:&lt;br /&gt;
Is the input directory on HDFS? On DB? How many files?&lt;/li&gt;
&lt;li&gt;Output URL:&lt;br /&gt;
Where will we store the output? On HDFS? On DB?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;interaction-between-mapreduce-framework-and-yarn-infrastructure&#34;&gt;Interaction between MapReduce Framework and YARN Infrastructure&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://xmxiaohuilin.github.io/media/hadoop/MR-Yarn.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;overview-of-map-reduce-function&#34;&gt;Overview of Map Reduce function&lt;/h3&gt;

&lt;p&gt;It is organized as a “map” function which transform a piece of data into some number of key/value pairs. Each of these elements will then be sorted by their key and reach to the same node, where a “reduce” function is use to merge the values (of the same key) into a single result.&lt;/p&gt;

&lt;h3 id=&#34;mapper&#34;&gt;Mapper&lt;/h3&gt;

&lt;p&gt;Mapper maps input key/value pairs to a set of intermediate key/value pairs.&lt;/p&gt;

&lt;p&gt;Maps are the individual tasks that transform input records into intermediate records. The transformed intermediate records do not need to be of the same type as the input records. A given input pair may map to zero or many output pairs.&lt;/p&gt;

&lt;p&gt;The following comes from one of Chinese blog:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;在map task执行时，它的输入数据来源于HDFS的block，当然在MapReduce概念中，map task只读取split。Split与block的对应关系可能是多对一，默认是一对一。在WordCount例子里，假设map的输入数据都是像“aaa”这样的字符串。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在经过mapper的运行后，我们得知mapper的输出是这样一个key/value对： key是“aaa”， value是数值1。因为当前map端只做加1的操作，在reduce task里才去合并结果集。前面我们知道这个job有3个reduce task，到底当前的“aaa”应该交由哪个reduce去做呢，是需要现在决定的。&lt;/p&gt;

&lt;p&gt;MapReduce提供Partitioner接口，它的作用就是根据key或value及reduce的数量来决定当前的这对输出数据最终应该交由哪个reduce task处理。默认对key hash后再以reduce task数量取模。默认的取模方式只是为了平均reduce的处理能力，如果用户自己对Partitioner有需求，可以订制并设置到job上。&lt;/p&gt;

&lt;p&gt;在我们的例子中，“aaa”经过Partitioner后返回0，也就是这对值应当交由第一个reducer来处理。接下来，需要将数据写入内存缓冲区中，缓冲区的作用是批量收集map结果，减少磁盘IO的影响。我们的key/value对以及Partition的结果都会被写入缓冲区。当然写入之前，key与value值都会被序列化成字节数组。&lt;/p&gt;

&lt;p&gt;整个内存缓冲区就是一个字节数组，它的字节索引及key/value存储结构我没有研究过。如果有朋友对它有研究，那么请大致描述下它的细节吧。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;这个内存缓冲区是有大小限制的，默认是100MB。当map task的输出结果很多时，就可能会撑爆内存，所以需要在一定条件下将缓冲区中的数据临时写入磁盘，然后重新利用这块缓冲区。这个从内存往磁盘写数据的过程被称为Spill，中文可译为溢写，字面意思很直观。这个溢写是由单独线程来完成，不影响往缓冲区写map结果的线程。溢写线程启动时不应该阻止map的结果输出，所以整个缓冲区有个溢写的比例spill.percent。这个比例默认是0.8，也就是当缓冲区的数据已经达到阈值（buffer size * spill percent = 100MB * 0.8 = 80MB），溢写线程启动，锁定这80MB的内存，执行溢写过程。Map task的输出结果还可以往剩下的20MB内存中写，互不影响。当溢写线程启动后，需要对这80MB空间内的key做排序(Sort)。排序是MapReduce模型默认的行为，这里的排序也是对序列化的字节做的排序。&lt;/p&gt;

&lt;p&gt;在这里我们可以想想，因为map task的输出是需要发送到不同的reduce端去，而内存缓冲区没有对将发送到相同reduce端的数据做合并，那么这种合并应该是体现是磁盘文件中的。从官方图上也可以看到写到磁盘中的溢写文件是对不同的reduce端的数值做过合并。所以溢写过程一个很重要的细节在于，如果有很多个key/value对需要发送到某个reduce端去，那么需要将这些key/value值拼接到一块，减少与partition相关的索引记录。&lt;/p&gt;

&lt;p&gt;在针对每个reduce端而合并数据时，有些数据可能像这样：“aaa”/1， “aaa”/1。对于WordCount例子，就是简单地统计单词出现的次数，如果在同一个map task的结果中有很多个像“aaa”一样出现多次的key，我们就应该把它们的值合并到一块，这个过程叫reduce也叫combine。但MapReduce的术语中，reduce只指reduce端执行从多个map task取数据做计算的过程。除reduce外，非正式地合并数据只能算做combine了。其实大家知道的，MapReduce中将Combiner等同于Reducer。&lt;/p&gt;

&lt;p&gt;如果client设置过Combiner，那么现在就是使用Combiner的时候了。将有相同key的key/value对的value加起来，减少溢写到磁盘的数据量。Combiner会优化MapReduce的中间结果，所以它在整个模型中会多次使用。那哪些场景才能使用Combiner呢？从这里分析，Combiner的输出是Reducer的输入，Combiner绝不能改变最终的计算结果。所以从我的想法来看，Combiner只应该用于那种Reduce的输入key/value与输出key/value类型完全一致，且不影响最终结果的场景。比如累加，最大值等。Combiner的使用一定得慎重，如果用好，它对job执行效率有帮助，反之会影响reduce的最终结果。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;每次溢写会在磁盘上生成一个溢写文件，如果map的输出结果真的很大，有多次这样的溢写发生，磁盘上相应的就会有多个溢写文件存在。当map task真正完成时，内存缓冲区中的数据也全部溢写到磁盘中形成一个溢写文件。最终磁盘中会至少有一个这样的溢写文件存在(如果map的输出结果很少，当map执行完成时，只会产生一个溢写文件)，因为最终的文件只有一个，所以需要将这些溢写文件归并到一起，这个过程就叫做Merge。Merge是怎样的？如前面的例子，“aaa”从某个map task读取过来时值是5，从另外一个map 读取时值是8，因为它们有相同的key，所以得merge成group。什么是group。对于“aaa”就是像这样的：{“aaa”, [5, 8, 2, …]}，数组中的值就是从不同溢写文件中读取出来的，然后再把这些值加起来。请注意，因为merge是将多个溢写文件合并到一个文件，所以可能也有相同的key存在，在这个过程中如果client设置过Combiner，也会使用Combiner来合并相同的key。&lt;br /&gt;
至此，map端的所有工作都已结束，最终生成的这个文件也存放在TaskTracker够得着的某个本地目录内。每个reduce task不断地通过RPC从JobTracker那里获取map task是否完成的信息，如果reduce task得到通知，获知某台TaskTracker上的map task执行完成，Shuffle的后半段过程开始启动。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;简单地说，reduce task在执行之前的工作就是不断地拉取当前job里每个map task的最终结果，然后对从不同地方拉取过来的数据不断地做merge，也最终形成一个文件作为reduce task的输入文件。&lt;/p&gt;

&lt;h3 id=&#34;reducer&#34;&gt;Reducer&lt;/h3&gt;

&lt;p&gt;Reducer reduces a set of intermediate values which share a key to a smaller set of values.&lt;/p&gt;

&lt;p&gt;The following comes from one of Chinese blog:
Reducer真正运行之前，所有的时间都是在拉取数据，做merge，且不断重复地在做。如前面的方式一样，下面我也分段地描述reduce 端的Shuffle细节：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Copy过程，简单地拉取数据。Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求map task所在的TaskTracker获取map task的输出文件。因为map task早已结束，这些文件就归TaskTracker管理在本地磁盘中。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Merge阶段。这里的merge如map端的merge动作，只是数组中存放的是不同map端copy来的数值。Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端的更为灵活，它基于JVM的heap size设置，因为Shuffle阶段Reducer不运行，所以应该把绝大部分的内存都给Shuffle用。这里需要强调的是，merge有三种形式：1)内存到内存  2)内存到磁盘  3)磁盘到磁盘。默认情况下第一种形式不启用，让人比较困惑，是吧。当内存中的数据量到达一定阈值，就启动内存到磁盘的merge。与map 端类似，这也是溢写的过程，这个过程中如果你设置有Combiner，也是会启用的，然后在磁盘中生成了众多的溢写文件。第二种merge方式一直在运行，直到没有map端的数据时才结束，然后启动第三种磁盘到磁盘的merge方式生成最终的那个文件。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Reducer的输入文件。不断地merge后，最后会生成一个“最终文件”。为什么加引号？因为这个文件可能存在于磁盘上，也可能存在于内存中。对我们来说，当然希望它存放于内存中，直接作为Reducer的输入，但默认情况下，这个文件是存放于磁盘中的。当Reducer的输入文件已定，整个Shuffle才最终结束。然后就是Reducer执行，把结果放到HDFS上。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;procedure&#34;&gt;Procedure&lt;/h3&gt;

&lt;h5 id=&#34;my-handwriting-procedure-about-how-mapreduce-works&#34;&gt;My handwriting procedure about how MapReduce works.&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmruibi/Sketchboard/master/map_reduce.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://4.bp.blogspot.com/_j6mB7TMmJJY/SS0CEJLklnI/AAAAAAAAAGQ/ogPGJ3WYpt4/s1600-h/P4.png&#34;&gt;Map Reduce Procedure Model&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://langyu.iteye.com/blog/992916&#34;&gt;MapReduce:详解Shuffle过程&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://hadoop.apache.org/docs/r2.6.1/index.html&#34;&gt;Hadoop Official Site&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to build a Hugo Site like this site?</title>
      <link>http://xmxiaohuilin.github.io/code/HugoSiteConstruction/</link>
      <pubDate>Mon, 12 Oct 2015 23:33:07 -0700</pubDate>
      
      <guid>http://xmxiaohuilin.github.io/code/HugoSiteConstruction/</guid>
      <description>

&lt;p&gt;This is how I build this site and the instruction for people who wanna use &lt;a href=&#34;http://gohugo.io/&#34;&gt;Hugo&lt;/a&gt; to build static site as Github Personal Page.&lt;/p&gt;

&lt;h2 id=&#34;what-is-hugo&#34;&gt;What is Hugo?&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Hugo is a general-purpose website framework. Technically speaking, Hugo is a static site generator.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Yes, Hugo is a kind of static site generator. Unlike the WordPress, Ghost, or Drupal, static web site doesn&amp;rsquo;t need to generate the page content when it received any request. So that means all pages should be set up once the site is built. Here is one more instruction for &lt;a href=&#34;https://www.udemy.com/build-static-sites-in-seconds-with-hugo/&#34;&gt;Hugo&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;what-should-you-learn&#34;&gt;What should you learn?&lt;/h2&gt;

&lt;p&gt;Hugo is built by Go language. Why use the Go? Author &lt;a href=&#34;http://spf13.com&#34;&gt;Steve Francia&lt;/a&gt; said:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I looked at existing static site generators like Jekyll, Middleman and nanoc. All had complicated dependencies to install and took far longer to render my blog with hundreds of posts than I felt was acceptable. I wanted a framework to be able to get rapid feedback while making changes to the templates, and the 5+-minute render times was just too slow. In general, they were also very blog minded and didn’t have the ability to have different content types and flexible URLs.&lt;/p&gt;

&lt;p&gt;I wanted to develop a fast and full-featured website framework without dependencies. The Go language seemed to have all of the features I needed in a language. I began developing Hugo in Go and fell in love with the language. I hope you will enjoy using (and contributing to) Hugo as much as I have writing it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Another thing you must know is MarkDown language, which is for writing your article. Please see the introduction of &lt;a href=&#34;https://en.wikipedia.org/wiki/Markdown&#34;&gt;Markdown&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;start-you-hugo-journey&#34;&gt;Start you Hugo journey!&lt;/h2&gt;

&lt;p&gt;Install by &lt;code&gt;brew&lt;/code&gt;! What? You don&amp;rsquo;t know that? Do the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ruby -e &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check you &lt;code&gt;brew&lt;/code&gt; has updated!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ brew update
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make sure your &lt;code&gt;brew&lt;/code&gt; has lastest &lt;code&gt;hugo&lt;/code&gt; version&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ brew info hugo
hugo: stable 0.14 (bottled), HEAD
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then start install &lt;code&gt;hugo&lt;/code&gt;!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ brew install hugo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Well done! Finished! You&amp;rsquo;ve install Hugo! But&amp;hellip;wait&amp;hellip;How to build a site?&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s go!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hugo new site yourFolder/siteFileName
$ cd /thatPath

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then you&amp;rsquo;ll see the Hugo structure like this!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; ▸ archetypes/ 
 ▸ content/
 ▸ layouts/
 ▸ static/
   config.toml

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;config.toml&lt;/code&gt; that is most important thing! That guides your configuration globally for your site, includes &lt;code&gt;baseurl&lt;/code&gt;, &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;copyright&lt;/code&gt;&amp;hellip;.&lt;/p&gt;

&lt;p&gt;For other folders:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;archetypes：for content type! make your rules for new generate .md content.&lt;/li&gt;
&lt;li&gt;content：for your articles. stores all your articles by using markdown format!&lt;/li&gt;
&lt;li&gt;layouts：for layout pattern. decide how your site showing structure. You can make your html as modules here.&lt;/li&gt;
&lt;li&gt;static：includes css, js, fonts, media.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So easy!&lt;/p&gt;

&lt;p&gt;Make a new article? Try this!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hugo new /post/yourArticle.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That will generate a &lt;code&gt;post&lt;/code&gt; folder in your &lt;code&gt;content&lt;/code&gt; folder and also a &lt;code&gt;yourArticle.md&lt;/code&gt; file!&lt;/p&gt;

&lt;p&gt;Try do write some thing in your article!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+++
date = &amp;quot;2015-02-01T18:19:54+08:00&amp;quot;
draft = true
title = &amp;quot;about&amp;quot;

+++

#About Me!

 - experience one
 - experience two
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;+++&lt;/code&gt; means the format for &lt;code&gt;.toml&lt;/code&gt; format. You can also use &lt;code&gt;---&lt;/code&gt; for &lt;code&gt;.yaml&lt;/code&gt; format.&lt;/p&gt;

&lt;h2 id=&#34;themes-for-hugo&#34;&gt;Themes for Hugo&lt;/h2&gt;

&lt;p&gt;However, there is nothing can show in your site. Because you don&amp;rsquo;t have the skin of your site! What is skin? That is the themes for Hugo! Theme for Hugo like the website template. That decides what your site look like! Hugo is convenient because your can easily to change your theme for entire site!&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s make a folder in your site root directory and go inside it!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir themes
$ cd themes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I used the &lt;code&gt;hyde-y&lt;/code&gt; as my theme. You can download it by:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/enten/hyde-y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then you&amp;rsquo;ll see a &lt;code&gt;themes/hyde-y&lt;/code&gt; folder, which contains similar structure like your site folder!&lt;/p&gt;

&lt;p&gt;You also can get more themes from &lt;a href=&#34;http://themes.gohugo.io/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;However, you still not have linked your site and this theme! Back to the root path in your site folder, get the &lt;code&gt;config.toml&lt;/code&gt; and add this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Theme to use (located in /themes/THEMENAME/)
theme = &amp;quot;hyde-y&amp;quot;

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It seems everything finished! Let&amp;rsquo;s try it!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hugo server -w
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then open &lt;code&gt;http://localhost:1313&lt;/code&gt;, we will see your site!&lt;/p&gt;

&lt;h2 id=&#34;add-more-function&#34;&gt;Add more function!&lt;/h2&gt;

&lt;h3 id=&#34;comment-for-your-article&#34;&gt;Comment for your article&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://disqus.com/&#34;&gt;Disqus&lt;/a&gt; is default supported by Hugo! Did you see that in &lt;code&gt;yourSite/themes/hyde-y/layouts/partials/modules/disqus.html&lt;/code&gt;? That is internal html module for show disqus comment below your article! However, you need to register your Disqus account! And set your disqus short name in your &lt;code&gt;config.toml&lt;/code&gt; file!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Enable Disqus integration
disqusShortname = &amp;quot;yourShortName&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;How to get your short name? Go your setting in Disqus profile home page and try &lt;code&gt;add disqus to your site&lt;/code&gt;, then follow the guide!&lt;/p&gt;

&lt;p&gt;Wait&amp;hellip; Doesn&amp;rsquo;t see the disqus module during using &lt;code&gt;localhost:1313&lt;/code&gt;?&lt;/p&gt;

&lt;p&gt;Go your  &lt;code&gt;yourSite/themes/hyde-y/layouts/partials/modules/disqus.html&lt;/code&gt; file, comment the following lines:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    // if (window.location.hostname == &amp;quot;localhost&amp;quot;)
    //   return;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Well you got it!&lt;/p&gt;

&lt;h3 id=&#34;push-on-github-home-page&#34;&gt;Push on Github Home Page&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Create repo for &lt;code&gt;your-username.github.io&lt;/code&gt;. Note that the repo name should be your username of your github. Only by this way you can make it as your personal home page.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Enter your project directory.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code&gt;cd workspace/yourProject&lt;/code&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Remove your &lt;code&gt;public&lt;/code&gt; folder under this directory. &lt;code&gt;rm -rf public&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Initilize this directory as github repo &lt;code&gt;git init&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Make &lt;code&gt;public/&lt;/code&gt; directory sync with .github.io&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code&gt;git submodule add git@github.com:&amp;lt;username&amp;gt;/&amp;lt;username&amp;gt;.github.io.git public&lt;/code&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Create a file &amp;lsquo;deploy.sh&amp;rsquo; under the your project dir and copy the following:
```
#!/bin/bash
echo -e &amp;ldquo;\033[0;32mDeploying updates to GitHub&amp;hellip;\033[0m&amp;rdquo;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;msg=&amp;ldquo;rebuilding site &lt;code&gt;date&lt;/code&gt;&amp;ldquo;
if [ $# -eq 1 ]
  then msg=&amp;ldquo;$1&amp;rdquo;
fi&lt;/p&gt;

&lt;h1 id=&#34;push-hugo-content&#34;&gt;Push Hugo content&lt;/h1&gt;

&lt;p&gt;git add -A
git commit -m &amp;ldquo;$msg&amp;rdquo;
git push origin master&lt;/p&gt;

&lt;h1 id=&#34;build-the-project&#34;&gt;Build the project.&lt;/h1&gt;

&lt;p&gt;hugo # if using a theme, replace by &lt;code&gt;hugo -t &amp;lt;yourtheme&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;h1 id=&#34;go-to-public-folder&#34;&gt;Go To Public folder&lt;/h1&gt;

&lt;p&gt;cd public&lt;/p&gt;

&lt;h1 id=&#34;add-changes-to-git&#34;&gt;Add changes to git.&lt;/h1&gt;

&lt;p&gt;git add -A&lt;/p&gt;

&lt;h1 id=&#34;commit-changes&#34;&gt;Commit changes.&lt;/h1&gt;

&lt;p&gt;git commit -m &amp;ldquo;$msg&amp;rdquo;&lt;/p&gt;

&lt;h1 id=&#34;push-source-and-build-repos&#34;&gt;Push source and build repos.&lt;/h1&gt;

&lt;p&gt;git push origin master&lt;/p&gt;

&lt;h1 id=&#34;come-back&#34;&gt;Come Back&lt;/h1&gt;

&lt;p&gt;cd ..
```&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Run the above .sh file
&lt;code&gt;./deploy.sh&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;try-more-i-ll-be-back-soon&#34;&gt;Try more? I&amp;rsquo;ll be back soon!&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>How to do the insite search in Hugo?</title>
      <link>http://xmxiaohuilin.github.io/code/BleveSearch/</link>
      <pubDate>Sat, 10 Oct 2015 23:56:15 -0700</pubDate>
      
      <guid>http://xmxiaohuilin.github.io/code/BleveSearch/</guid>
      <description>

&lt;p&gt;Since I tried to avoid using the Google tool, searching insite content in static site like Hugo seems a tough thing for me.  However, I just found &lt;a href=&#34;https://github.com/blevesearch/hugoidx&#34;&gt;Bleeve Search&lt;/a&gt;, which is a great tool to assist the insite search.&lt;/p&gt;

&lt;p&gt;There are three steps to adding search to your site. First, you must build the index. Second, you must host the index. Third, you add a search page to your site.&lt;/p&gt;

&lt;h3 id=&#34;building-the-index&#34;&gt;Building the Index&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Preparation: Check you have installed Go. Two ways to install Go, see the instruction in &lt;a href=&#34;https://golang.org/dl/&#34;&gt; Download GO&lt;/a&gt;. Also, be awared to the GOPATH&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export %GOPATH = &amp;quot;.../...&amp;quot;
source etc/profile
echo $GOPATH
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Be sured you&amp;rsquo;ve also installed Mercurial. Check it by command &lt;code&gt;hg&lt;/code&gt;. You can use &lt;code&gt;brew&lt;/code&gt; to install it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install hg
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Install &lt;strong&gt;hugoidx&lt;/strong&gt; - this is the command we will use build the search index.  Anytime you update your content and regenerate your site using the &lt;code&gt;hugo&lt;/code&gt; command, you&amp;rsquo;ll also want to rebuild your search index.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go get github.com/blevesearch/hugoidx
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;cd &amp;lt;your hugo site&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;hugoidx&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You should now have a file named &lt;code&gt;search.bleve&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;hosting-the-index&#34;&gt;Hosting the Index&lt;/h3&gt;

&lt;p&gt;In order to host the index we need to run a small Go program that is available on the internet.  To simplify this process, we have built a reusable application called &lt;code&gt;bleve-hosted&lt;/code&gt;.  You can use this application safely answer queries to the index (read-only operations).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Install &lt;code&gt;bleve-hosted&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go get github.com/blevesearch/bleve-hosted
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;cd $GOPATH/src/github.com/blevesearch/bleve-hosted&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;bleve-hosted&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Test that its working:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl http://localhost:8080/api/test.bleve/_search -d &#39;{&amp;quot;query&amp;quot;:{&amp;quot;query&amp;quot;:&amp;quot;bleve&amp;quot;}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The resulting JSON should include &amp;ldquo;total_hits&amp;rdquo;: 1&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Copy the &lt;code&gt;search.bleve&lt;/code&gt; index you generated earlier into your &lt;code&gt;indexes/&lt;/code&gt; folder.  (This can really be anywhere, it will always look for an &lt;code&gt;indexes/&lt;/code&gt; folder relative to the current working directly when you launch &lt;code&gt;bleve-hosted&lt;/code&gt;.)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Restart &lt;code&gt;bleve-hosted&lt;/code&gt; and optionally configure your server to keep this process running long term (init-scripts, etc)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;add-search-to-your-site&#34;&gt;Add Search to your Site&lt;/h3&gt;

&lt;p&gt;Finally, we&amp;rsquo;re ready to add a search page to our site.  Several files were downloaded as a part of the &lt;code&gt;hugoidx&lt;/code&gt; package to help you get started.  Feel free to customize these files to best adapt them to your site.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;cd &amp;lt;your hugo site&amp;gt;&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Copy the main search page:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp $GOPATH/src/github.com/blevesearch/hugoidx/search.md content/
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Check and copy two Javascript files in my Github:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://github.com/xmruibi/xmruibi.github.io/blob/master/js/handlebars.js
https://github.com/xmruibi/xmruibi.github.io/blob/master/js/search.js
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Copy these two files into your &lt;code&gt;static/&lt;/code&gt; folder. Also, make sure you&amp;rsquo;ve &lt;code&gt;jquery.min.js&lt;/code&gt; in this folder.&lt;/p&gt;

&lt;p&gt;handlebars.js is used to render search results using a simple template syntax.&lt;br /&gt;
search.js is our custom code to bind everything together.&lt;/p&gt;

&lt;p&gt;jQuery is used to make AJAX requests from the browser to &lt;code&gt;bleve-hosted&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Update your layout to include these javascript files.  For many sites this will be in a file like &lt;code&gt;layouts/partial/footer.html&lt;/code&gt; or &lt;code&gt;themes/&amp;lt;your theme&amp;gt;/layouts/partials/footer.html&lt;/code&gt;.  In the section where javascript files are being included you&amp;rsquo;ll want to add:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;script src=&amp;quot;/js/jquery.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script src=&amp;quot;/js/handlebars.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script src=&amp;quot;/js/search.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Finally, we need to update search.js to point to the correct URL for &lt;code&gt;bleve-hosted&lt;/code&gt;.  On line 2 of &lt;code&gt;static/js/search.js&lt;/code&gt; modify the value:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var searchURL = &#39;http://&amp;lt;your server&amp;gt;:8080/api/search.bleve/_search&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;touch-the-search-function&#34;&gt;Touch the Search function&lt;/h3&gt;

&lt;p&gt;You need to setup file &lt;code&gt;search.html&lt;/code&gt; in &lt;code&gt;layout/partials/modules/site/link&lt;/code&gt;, which is for the search bar in navgation sidebar. And also a &lt;code&gt;search.md&lt;/code&gt; file in your content folder.&lt;/p&gt;

&lt;p&gt;Here provided a great CSS to generate the beautiful search bar. Please check &lt;a href =&#34;https://github.com/xmruibi/xmruibi.github.io/blob/master/css/search.css&#34;&gt; Code &lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Make you search form includes both components:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    &amp;lt;input id=&amp;quot;page&amp;quot; name=&amp;quot;p&amp;quot; value=&amp;quot;1&amp;quot; type=&amp;quot;hidden&amp;quot;/&amp;gt;
    &amp;lt;input id=&amp;quot;query&amp;quot; name=&amp;quot;q&amp;quot; type=&amp;quot;search&amp;quot; placeholder=&amp;quot;Search&amp;quot; /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Spring Boot App with Elastic Search Indexing Service</title>
      <link>http://xmxiaohuilin.github.io/code/ElasticSearch/</link>
      <pubDate>Mon, 10 Aug 2015 23:56:15 -0700</pubDate>
      
      <guid>http://xmxiaohuilin.github.io/code/ElasticSearch/</guid>
      <description>

&lt;p&gt;This article shows an example to build a bridge between Spring-Boot App and Elastic Search Indexing Service.&lt;/p&gt;

&lt;h2 id=&#34;1-elastic-search-install&#34;&gt;1. Elastic Search Install&lt;/h2&gt;

&lt;p&gt;Install elastic search is very easy&lt;/p&gt;

&lt;h3 id=&#34;install-elastic-search&#34;&gt;Install Elastic Search&lt;/h3&gt;

&lt;h4 id=&#34;start-elastic-search&#34;&gt;Start Elastic Search&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;    # windows: cd yourPath: service install
                            service start
                            service stop
    # Mac: cd $ELASTIC_HOME: elasticsearch
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;check-your-service-by-input-the-url-localhost-9200-in-your-browser&#34;&gt;Check your service by input the url &lt;code&gt;localhost:9200&lt;/code&gt; in your browser.&lt;/h4&gt;

&lt;h4 id=&#34;cluster-name&#34;&gt;Cluster Name&lt;/h4&gt;

&lt;p&gt;If cluster name is not &amp;ldquo;elasticsearch&amp;rdquo;, it may cause the run failed when your Java code trying to build elastic search instance. There should be an exception( NoNodeClientException: None of node configed) when your indexing the data.&lt;/p&gt;

&lt;p&gt;Please change your cluster name into &amp;ldquo;elasticsearch&amp;rdquo; in your elasticsearch install path.
 $ELASTIC_HOME/config/elasticsearch.yml : clustername = elasticsearch&lt;/p&gt;

&lt;h2 id=&#34;2-creat-index-by-spring-data-elasticsearch-api&#34;&gt;2. Creat Index by Spring-data-elasticsearch API&lt;/h2&gt;

&lt;p&gt;Here is a example from my project. It shows how I creat an index for music by Bulk method, which is provided by Elastic Java API.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;@Service
public class MusicIndexingService {

	private final static Logger logger = Logger
			.getLogger(MusicIndexingService.class);

	@Autowired	
	private Client client ;
	
	private ObjectMapper mapper = new ObjectMapper();

	/**
	 * The bulk index method
	 * @param musicCollection for index
	 */
	public void bulkIndex(List&amp;lt;IndexedMusic&amp;gt; musicCollection) {
		client.delete(new DeleteRequest(&amp;quot;musics&amp;quot;));
		logger.info(&amp;quot;Indexing bulk request of &amp;quot; + musicCollection.size()
				+ &amp;quot; documents&amp;quot;);
		BulkRequestBuilder bulkRequest = client.prepareBulk();
		for (IndexedMusic music : musicCollection) {
			String json = null;
			try {
				json = mapper.writeValueAsString(music);
			} catch (JsonProcessingException e) {
				throw new RuntimeException(e);
			}
			bulkRequest.add(client.prepareIndex(&amp;quot;musics&amp;quot;, &amp;quot;music&amp;quot;,
					UUID.randomUUID().toString()).setSource(json));
		}
		BulkResponse response = bulkRequest.execute().actionGet();
		if (response.hasFailures()) {
			throw new RuntimeException(
					&amp;quot;there was an error indexing the bulk request of &amp;quot;
							+ musicCollection.size() + &amp;quot; documents: &amp;quot; +response.buildFailureMessage());
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;## 3. Search the index:
 Search a music by name.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;/**
	 * This is keyword search method in comment contents field in music object
	 * @param keyword
	 * @return
	 */
	public List&amp;lt;Music&amp;gt; findMusic(String keyword) {
		QueryBuilder matchquery = QueryBuilders.fuzzyLikeThisFieldQuery(
				&amp;quot;commentContents&amp;quot;).likeText(keyword);
		SearchRequestBuilder requestBuilder = client.prepareSearch(&amp;quot;musics&amp;quot;)
				.setQuery(matchquery);
		SearchResponse response = requestBuilder.execute().actionGet();
		SearchHits hits = response.getHits();
		List&amp;lt;String&amp;gt; musicIdsList = new ArrayList&amp;lt;String&amp;gt;();
		Iterator&amp;lt;SearchHit&amp;gt; iterator = hits.iterator();
		while (iterator.hasNext()) {
			musicIdsList.add(iterator.next().getSource().get(&amp;quot;id&amp;quot;).toString());
		}
		return (List&amp;lt;Music&amp;gt;) musicRepository.findAll(musicIdsList);
	}

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Architecture Design on Social Music Search project</title>
      <link>http://xmxiaohuilin.github.io/code/SpringBoot/</link>
      <pubDate>Sun, 09 Aug 2015 10:56:15 -0700</pubDate>
      
      <guid>http://xmxiaohuilin.github.io/code/SpringBoot/</guid>
      <description>

&lt;h2 id=&#34;framework&#34;&gt;Framework&lt;/h2&gt;

&lt;h4 id=&#34;why-spring-boot&#34;&gt;Why Spring Boot?&lt;/h4&gt;

&lt;p&gt;Spring framework goes every where in current enterprise application. However, most of people are familiar with Spring MVC. Here I just want to introduce a new Spring Boot project. Hear what its official document said:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can &amp;ldquo;just run&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Yes, unlike Spring MVC, the Spring Boot require less configuration and easier to deploy on remote virtual machine or cloud computing platform. Spring Boot has following features:&lt;/p&gt;

&lt;h4 id=&#34;features&#34;&gt;Features&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Create stand-alone Spring applications&lt;/li&gt;
&lt;li&gt;Embed Tomcat, Jetty or Undertow directly (no need to deploy WAR files)&lt;/li&gt;
&lt;li&gt;Provide opinionated &amp;lsquo;starter&amp;rsquo; POMs to simplify your Maven configuration&lt;/li&gt;
&lt;li&gt;Automatically configure Spring whenever possible&lt;/li&gt;
&lt;li&gt;Provide production-ready features such as metrics, health checks and externalized configuration&lt;/li&gt;
&lt;li&gt;Absolutely no code generation and no requirement for XML configuration&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;three-layers-should-be-enough-for-everybody&#34;&gt;Three Layers Should Be Enough for Everybody&lt;/h4&gt;

&lt;p&gt;If think about the responsibilities of a web application, we notice that a web application has the following “concerns”:&lt;/p&gt;

&lt;p&gt;It needs to process the user’s input and return the correct response back to the user.&lt;br /&gt;
It needs an exception handling mechanism that provides reasonable error messages to the user.&lt;br /&gt;
It needs a transaction management strategy.
It needs to handle both authentication and authorization.&lt;br /&gt;
It needs to implement the business logic of the application.&lt;br /&gt;
It needs to communicate with the used data storage and other external resources.&lt;/p&gt;

&lt;p&gt;We can fulfil all these concerns by using “only” three layers. These layers are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The web layer&lt;/strong&gt; is the uppermost layer of a web application. It is responsible of processing user’s input and returning the correct response back to the user. The web layer must also handle the exceptions thrown by the other layers. Because the web layer is the entry point of our application, it must take care of authentication and act as a first line of defense against unauthorized users.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The service layer&lt;/strong&gt; resides below the web layer. It acts as a transaction boundary and contains both application and infrastructure services. The application services provides the public API of the service layer. They also act as a transaction boundary and are responsible of authorization. The infrastructure services contain the “plumbing code” that communicates with external resources such as file systems, databases, or email servers. Often these methods are used by more than a one application service.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The repository layer&lt;/strong&gt; is the lowest layer of a web application. It is responsible of communicating with the used data storage.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://xmxiaohuilin.github.io/media/spring/spring-web-application-layers.png&#34;&gt;&lt;/p&gt;

&lt;h2 id=&#34;backend-design-detail&#34;&gt;Backend Design Detail&lt;/h2&gt;

&lt;p&gt;Here is how I design my Social Music Search project with Spring Boot:&lt;/p&gt;

&lt;h3 id=&#34;restful-serivce&#34;&gt;RESTful Serivce&lt;/h3&gt;

&lt;p&gt;Do the backend and frontend communication&lt;/p&gt;

&lt;h5 id=&#34;spring-data-rest&#34;&gt;Spring-Data-REST&lt;/h5&gt;

&lt;p&gt;Package:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Config: ApplicationConfig;&lt;/li&gt;
&lt;li&gt;controller: call services and tranfer data as JSON format to the frondend&lt;/li&gt;
&lt;li&gt;service: call different service from mongodb or elastic search&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;database-service&#34;&gt;Database Service&lt;/h3&gt;

&lt;p&gt;Non-SQL database, MongoDB, to be the database solution&lt;/p&gt;

&lt;h5 id=&#34;spring-data-mongodb&#34;&gt;Spring-Data-MongoDB&lt;/h5&gt;

&lt;p&gt;Package:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Config: MongoDBConfig;&lt;/li&gt;
&lt;li&gt;mongodb.service&lt;/li&gt;
&lt;li&gt;mongodb.repository;&lt;/li&gt;
&lt;li&gt;domain: Music, BulletComment, User, Genre&amp;hellip;;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;indexing-service&#34;&gt;Indexing service&lt;/h3&gt;

&lt;p&gt;Achieve the advanced search function&lt;/p&gt;

&lt;h5 id=&#34;spring-data-elasticsearch&#34;&gt;Spring-Data-ElasticSearch&lt;/h5&gt;

&lt;p&gt;Package:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Config: ElasticSearchConfig; (port:9300)&lt;/li&gt;
&lt;li&gt;index.service&lt;/li&gt;
&lt;li&gt;index.repository;&lt;/li&gt;
&lt;li&gt;index.domain: Indexed Music;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;spring-boot-app-configuration&#34;&gt;Spring Boot App Configuration&lt;/h2&gt;

&lt;p&gt;The Spring Boot requires some basic configuration and set up the bootstrap entrance:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It doesn&amp;rsquo;t need &lt;code&gt;web.xml&lt;/code&gt; whic is common for Spring MVC;&lt;/li&gt;
&lt;li&gt;Set up the bootstrap by Maven plugin.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Bootstrap Main function (Entrance of Spring Boot App): MusicSearchApplication;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Spring Configuration (config package)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ApplicationConfig
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;@Configuration
@PropertySource(&amp;quot;classpath:application.properties&amp;quot;) 
    // point out the application.properties as configuration source
public class ApplicationConfig {
    public @Bean LoggingEventListener mongoEventListener() {
        return new LoggingEventListener();
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;WebMVCConfig
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;@Configuration
@ComponentScan({&amp;quot;com.musicSearch.core.controller&amp;quot;,
    &amp;quot;com.musicSearch.core.service&amp;quot;, &amp;quot;com.musicSearch.core.domain&amp;quot;}) 
// here is important to do component scan
public class WebMVCConfig extends WebMvcConfigurerAdapter {
    @Override
    public void addViewControllers(ViewControllerRegistry registry) {
        registry.addViewController(&amp;quot;/static&amp;quot;)
                .setViewName(&amp;quot;forward:/index.html&amp;quot;);
        // point out the .css/.js or other static files target and default home page
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;ApplicationInitializer: Core entrance configuration
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;@Configuration
@EnableAutoConfiguration
@Import({ MongoDBConfig.class, ElasticSearchConfig.class,
        ApplicationConfig.class, WebMVCConfig.class,
        RepositoryRestMvcConfiguration.class })
public class MusicSearchApplication extends SpringBootServletInitializer {
    public static void main(String[] args) {
        SpringApplication.run(MusicSearchApplication.class, args);
    }
    @Override
    protected SpringApplicationBuilder configure(
            SpringApplicationBuilder application) {
        return application.sources(MusicSearchApplication.class);
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://projects.spring.io/spring-boot/&#34;&gt;Spring Official Site&lt;/a&gt;
&lt;/br&gt;
&lt;a href=&#34;http://www.petrikainulainen.net/software-development/design/understanding-spring-web-application-architecture-the-classic-way/&#34;&gt;Understanding Spring Web Application Architecture: The Classic Way&lt;/a&gt;
&lt;/br&gt;
&lt;a href=&#34;http://martinfowler.com/eaaCatalog/index.html&#34;&gt;Patterns of Enterprise Application Architecture&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to Input and Output with Mongo DB Data on Hadoop Platform</title>
      <link>http://xmxiaohuilin.github.io/code/MongoDBwithHadoop/</link>
      <pubDate>Thu, 16 Apr 2015 12:56:15 -0700</pubDate>
      
      <guid>http://xmxiaohuilin.github.io/code/MongoDBwithHadoop/</guid>
      <description>

&lt;p&gt;In the previous article, I&amp;rsquo;ve talked about how to distributed a Mongo DB database in multiple virtual machine. Here, I&amp;rsquo;d like to discuss how we can make data stream between Mongo DB and Hadoop. In fact, the format from MongoDB cannot directly used in Hadoop Map Reduce function. You have to use Mongo Hadoop API to assist you for finishing this difficult connection between MongoDB and Hadoop.&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The MongoDB Connector for Hadoop is a plugin for Hadoop that provides the ability to use MongoDB as an input source and/or an output destination. The major advantage is that once the connector is implemented, the analytic power of Hadoop can be utilized with the MongoDB storage architecture.
&amp;gt; “The Connector presents MongoDB as a Hadoop-compatible file system allowing a MapReduce job to read from MongoDB directly without first copying it to HDFS, thereby eliminating the need to move Terabytes of data across the network. MapReduce jobs can pass queries as filters, so avoiding the need to scan entire collections, and can also take advantage of MongoDB’s rich indexing capabilities including geospatial, text-search, array, compound and sparse indexes”&lt;/p&gt;

&lt;h2 id=&#34;procedure&#34;&gt;Procedure&lt;/h2&gt;

&lt;p&gt;Once we choose MongoDB as the I/O target, all of our I/O format needs to fit with MongoDB document format, thus we must use either JSON or BSON. So the entire procedure when Hadoop works with MongoDB is shown as follows:
- Create MongoDB URL builder with multiple Mongos (Routers) as Database entrances
- Set one MongoDB collection as input source with MongoDB URL builder
- Use Hadoop as Map-Reduce computing framework.
- Override Mapper class with BSON format as value in.
- Override Reduce class addressing the BSON data and output the data with MongodbUpdateWritable format
- Set another MongoDB collection as output destination  with MongoDB URL builder&lt;/p&gt;

&lt;h2 id=&#34;talk-is-cheap-show-me-the-code&#34;&gt;Talk is cheap, show me the code!&lt;/h2&gt;

&lt;h3 id=&#34;access-to-mongo-db&#34;&gt;Access to Mongo DB&lt;/h3&gt;

&lt;p&gt;By create MongoDB URL builder, the hadoop platform can detect the database position. However, we have multiple Mongo DB routers as Database entrances. So we just save our entry IP address as a list.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private MongoClientURIBuilder ShardedDBURIBuilder() {
		// mainly host
		String mainHost = &amp;quot;44XX.XXX.XXX.X01&amp;quot;;

		// set up the option entrances when the main one shut down
		List&amp;lt;ServerAddress&amp;gt; serverSeeds;
		MongoClient mongoClient = null;
		serverSeeds = new ArrayList&amp;lt;ServerAddress&amp;gt;();
		try {
			serverSeeds.add(new ServerAddress(&amp;quot;4XX.XXX.XXX.X02&amp;quot;, 27017));
			serverSeeds.add(new ServerAddress(&amp;quot;4XX.XXX.XXX.X03&amp;quot;, 27017));
			mongoClient = new MongoClient(serverSeeds);
			mongoClient.getMongoClientOptions();
		} catch (UnknownHostException e) {
			log.error(e + &amp;quot;&amp;quot; + e.getCause());
		}

		MongoClientURIBuilder uriBuilder = new MongoClientURIBuilder();
		uriBuilder.addHost(mainHost, 27017);
		uriBuilder.options(mongoClient.getMongoClientOptions());
		return uriBuilder;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we know where is the Mongo DB! But we still need to locate the collection in MongoDB and more detailed information. Here we go:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;MongoClientURIBuilder uriBuilder = ShardedDBURIBuilder();
uriBuilder.collection(&amp;quot;stock&amp;quot;, &amp;quot;symbols&amp;quot;);
MongoClientURI inputURI = uriBuilder.build();
MongoConfigUtil.setInputURI(getConf(), inputURI);
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;build-our-own-mapper-and-reducer&#34;&gt;Build our own mapper and reducer!&lt;/h3&gt;

&lt;p&gt;Here I use my project about stock information crawler as an example. Mapper read the stock symbol data from the MongoDB. Then it get a list of symbol name in mapper. The mapper has separated that list and the reducer should read different part of list and search the stock information according to the partial symbol name list.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Override Mapper class with BSON format as value in.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public class SymbolsMapper extends Mapper&amp;lt;Object, BSONObject, Text, IntWritable&amp;gt;{
    @Override
    public void map(Object key, BSONObject val, final Context context) 
        throws IOException, InterruptedException {
    	System.out.println(val.get(&amp;quot;symbol&amp;quot;)+&amp;quot;  Mapper Getted!!&amp;quot;);
        context.write(new Text((val.get(&amp;quot;symbol&amp;quot;)).toString()), 
        		new IntWritable(1));
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Override Reduce class addressing the BSON data and output the data with MongodbUpdateWritable format&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public class SymbolsReducer extends Reducer&amp;lt;Text, IntWritable, NullWritable, MongoUpdateWritable&amp;gt;{
	
    @Override
    public void reduce(final Text pKey, final Iterable&amp;lt;IntWritable&amp;gt; pValues,
                        final Context pContext )
            throws IOException, InterruptedException{
    	    
        StockCrawler stockCrawler = new StockCrawler();
        
        // get symbol from keyIn 
        Quote quote = stockCrawler.getHistQuotesBySymbol(pKey.toString());
        if(quote==null||quote.getSymbolName()==null||quote.getHistorical_quotes()==null)
        	return;
        // get Quote info, set new id
        BasicBSONObject query = new BasicBSONObject(&amp;quot;_id&amp;quot;, quote.getKey());
       
        // set symbol name and symbol quotes
        BasicBSONObject stockQuote = new BasicBSONObject();
        stockQuote.put(&amp;quot;symbol&amp;quot;, quote.getSymbolName());
        ArrayList&amp;lt;Object&amp;gt; historical_quotes =  quote.getHistorical_quotes();
        
        BasicBSONObject update = new BasicBSONObject(&amp;quot;$set&amp;quot;, stockQuote);
        update.append(&amp;quot;$pushAll&amp;quot;, new BasicBSONObject(&amp;quot;historical_quotes&amp;quot;, historical_quotes));
        
        pContext.write(null, new MongoUpdateWritable(query, update, true, false));
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;output-back-to-mongo-db&#34;&gt;Output back to Mongo DB&lt;/h3&gt;

&lt;p&gt;The output procedure is very simliar with the input one. But just the difference on the target collection in Mongo DB.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;MongoClientURI outputURI = uriBuilder.build();
MongoConfigUtil.setOutputURI(getConf(), outputURI);
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;the-overview-on-this-connector&#34;&gt;The overview on this connector:&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public MongoMapredStockCrawler() {
		setConf(new Configuration());

		if (MongoTool.isMapRedV1()) {
			MapredMongoConfigUtil.setInputFormat(getConf(),
					com.mongodb.hadoop.mapred.MongoInputFormat.class);
			MapredMongoConfigUtil.setOutputFormat(getConf(),
					com.mongodb.hadoop.mapred.MongoOutputFormat.class);
		} else {
			MongoConfigUtil.setInputFormat(getConf(), MongoInputFormat.class);
			MongoConfigUtil.setOutputFormat(getConf(), MongoOutputFormat.class);
		}

		MongoClientURIBuilder uriBuilder = ShardedDBURIBuilder();
		uriBuilder.collection(&amp;quot;stock&amp;quot;, &amp;quot;symbols&amp;quot;);
		MongoClientURI inputURI = uriBuilder.build();
		uriBuilder.collection(&amp;quot;stock&amp;quot;, &amp;quot;quotes&amp;quot;);
		MongoClientURI outputURI = uriBuilder.build();

		MongoConfigUtil.setInputURI(getConf(), inputURI);
		MongoConfigUtil.setOutputURI(getConf(), outputURI);

		MongoConfigUtil.setMapper(getConf(), SymbolsMapper.class);
		MongoConfigUtil.setReducer(getConf(), SymbolsReducer.class);

		MongoConfigUtil.setMapperOutputKey(getConf(), Text.class);
		MongoConfigUtil.setMapperOutputValue(getConf(), IntWritable.class);

		MongoConfigUtil.setOutputKey(getConf(), IntWritable.class);
		MongoConfigUtil.setOutputValue(getConf(), BSONWritable.class);
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This is very typical example when people need to read data from Mongo DB and process the data on Hadoop platform and then back the data to Mongo DB. Since there is very little instruction about this work, I just shared my idea on that.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Distributed MongoDB Configuration</title>
      <link>http://xmxiaohuilin.github.io/code/DistributedMongoDB/</link>
      <pubDate>Tue, 10 Mar 2015 22:56:15 -0700</pubDate>
      
      <guid>http://xmxiaohuilin.github.io/code/DistributedMongoDB/</guid>
      <description>

&lt;p&gt;How to shard the distributed Mongo DB in remote VMs? Here is what I did during a project using Mongo DB as Database and using Hadoop as computing framework.&lt;/p&gt;

&lt;h2 id=&#34;sharded-mongodb-configuration&#34;&gt;Sharded MongoDB Configuration&lt;/h2&gt;

&lt;p&gt;The following graph is the architecture of how I set three VMs with different port to simulate the real sharding pattern(which need 15 machines actually)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://xmxiaohuilin.github.io/media/Sharded%20MongoDB.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;

&lt;p&gt;The following procedure is how I configured MongoDB on remote three VMs.&lt;/p&gt;

&lt;h3 id=&#34;1-set-up-data-path-config-file-and-log-file-paths-in-each-node-with-mongos-config-shard1-shard2-shard3-directory-name&#34;&gt;1. Set up data path, config file and log file paths in each node with mongos 、config 、 shard1 、shard2、shard3 (directory name)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /data/mongos/log
sudo chmod -R 777 /data/mongos/log
mkdir -p /data/config/data
sudo chmod -R 777 /data/config/data
mkdir -p /data/config/log
sudo chmod -R 777 /data/config/log
mkdir -p /data/mongos/log
sudo chmod -R 777 /data/mongos/log
mkdir -p /data/shard1/data
sudo chmod -R 777 /data/shard1/data
mkdir -p /data/shard1/log
sudo chmod -R 777 /data/shard1/log
mkdir -p /data/shard2/data
sudo chmod -R 777 /data/shard2/data
mkdir -p /data/shard2/log
sudo chmod -R 777 /data/shard2/log
mkdir -p /data/shard3/data
sudo chmod -R 777 /data/shard3/data
mkdir -p /data/shard3/log
sudo chmod -R 777 /data/shard3/log
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-make-a-plan-of-port-number-and-modify-some-config-parameter-in-mongod-conf&#34;&gt;2. Make a plan of port number and modify some config parameter in mongod.conf&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;mongod --configsvr --dbpath /data/config/data --port 27019 --logpath /data/config/log/config.log --fork


mongos  --configdb 45.55.188.234:27019,45.55.186.238:27019,104.131.106.22:27019  --port 27017   --logpath  /data/mongos/log/mongos.log --fork

## or 
## These codes can migration configuration 

rsync -az /data/configdb mongo-config1.example.net:/data/configdb
rsync -az /data/configdb mongo-config2.example.net:/data/configdb

nano etc/mongod.conf

### This is important!!
set bing_ip = 0.0.0.0; for remote login
set default mongod: 27019 
## !Otherwise your mongos port(27017) will be blocked
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-config-sharding-setting-on-each-vm&#34;&gt;3. Config sharding setting on each VM&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;### set up shards ports and dbpath and log path
mongod --shardsvr --replSet shard1 --port 22001 --dbpath /data/shard1/data  --logpath /data/shard1/log/shard1.log --fork --journal  --oplogSize 10
mongod --shardsvr --replSet shard2 --port 22002 --dbpath /data/shard2/data  --logpath /data/shard2/log/shard2.log --fork --journal  --oplogSize 10
mongod --shardsvr --replSet shard3 --port 22003 --dbpath /data/shard3/data  --logpath /data/shard3/log/shard3.log --fork --journal  --oplogSize 10


# Shard_1 in Node(45.55.188.234)
mongo  127.0.0.1:22001
use admin
config = { _id:&amp;quot;shard1&amp;quot;, members:[
                     {_id:0,host:&amp;quot;45.55.188.234:22001&amp;quot;},
                     {_id:1,host:&amp;quot;45.55.186.238:22001&amp;quot;},
                {_id:2,host:&amp;quot;104.131.106.22:22001&amp;quot;,arbiterOnly:true}
                ]
         }
rs.initiate(config);

# Shard_2 in Node(45.55.186.238)
mongo  127.0.0.1:22002
use admin
config = { _id:&amp;quot;shard2&amp;quot;, members:[
                     {_id:0,host:&amp;quot;45.55.186.238:22002&amp;quot;},
                     {_id:1,host:&amp;quot;104.131.106.22:22002&amp;quot;},
                {_id:2,host:&amp;quot;45.55.188.234:22002&amp;quot;,arbiterOnly:true}
                ]
         }
rs.initiate(config);

# Shard_3 in Node(104.131.106.22)
mongo  127.0.0.1:22003
use admin
config = { _id:&amp;quot;shard3&amp;quot;, members:[
                     {_id:0,host:&amp;quot;104.131.106.22:22003&amp;quot;},
                     {_id:1,host:&amp;quot;45.55.188.234:22003&amp;quot;},
           {_id:2,host:&amp;quot;45.55.186.238:22003&amp;quot;,arbiterOnly:true}
                ]
         }
rs.initiate(config);
## if you need to reconfig, please use Cmd( rs.reconfig(your_para) )
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-add-shard-config-just-in-one-of-vms&#34;&gt;4. Add Shard Config just in one of VMs&lt;/h3&gt;

&lt;p&gt;It seems no master mode concept in MongoDB. So just choose one of it. Config Sharding info in mongos; I also made the sharding part on different machines (e.g. Primary Shard1 on Server One, Primary Shard2 on Server Two).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## no specific addr/port 
mongo
use admin

db.runCommand({addshard : &amp;quot;shard1/45.55.188.234:22001,45.55.186.238:22001,104.131.106.22:22001&amp;quot;});

db.runCommand({addshard: &amp;quot;shard2/45.55.186.238:22002,104.131.106.22:22002,45.55.188.234:22002&amp;quot;});

db.runCommand({addshard : &amp;quot;shard3/104.131.106.22:22003,45.55.188.234:22003,45.55.186.238:22003&amp;quot;});

## if you need to reset your previous setting
## 
db.runCommand( { removeShard: &amp;quot;shard1&amp;quot; } )

#Test:
db.runCommand( { enablesharding :&amp;quot;stock&amp;quot;});
db.runCommand( { shardcollection : &amp;quot;stock.quotes&amp;quot;,key : {&amp;quot;_id&amp;quot;: 1} })

for (var i = 1; i &amp;lt;= 100000; i++) db.table1.save({id:i,&amp;quot;test1&amp;quot;:&amp;quot;testval1&amp;quot;});


use admin
db.addUser(&#39;test&#39;,&#39;test&#39;)
db.auth(&#39;test&#39;,&#39;test&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-some-commmands-for-check-database-status&#34;&gt;5. Some Commmands for check Database Status;&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;db.stats();

show databases

db.dropDatabase()

db.printShardingStatus()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;6-misc&#34;&gt;6. Misc.&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;## Sometimes export jar failed
 zip -d stockCrawler.jar META-INF/LICENSE
 jar tvf stockCrawler.jar | grep -i license
 
 ## HDFS manipulation
 
 hadoop fs -ls
 hadoop fs -mkdir /user/${adminName}   
 hadoop fs -touch test
 hdfs dfs -copyFromLocal ${fileName}
 hdfs dfs -cat ${fileName}
 hadoop fs -rmr output
 hadoop jar stockCrawler.jar
 
 ## Some query example:
 db.quotes.find({&#39;historical_quotes.date&#39;:&#39;2015-04-10&#39;})
 
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning: Query Classifier</title>
      <link>http://xmxiaohuilin.github.io/code/QueryClassifier/</link>
      <pubDate>Mon, 12 Jan 2015 15:33:07 -0700</pubDate>
      
      <guid>http://xmxiaohuilin.github.io/code/QueryClassifier/</guid>
      <description>

&lt;p&gt;A system for figuring out the category of user query in search engine and potential subtask of user in group searching task.&lt;/p&gt;

&lt;h2 id=&#34;basically&#34;&gt;Basically&lt;/h2&gt;

&lt;p&gt;Query classifier is some kind of that when you give a query then it return the most possible category or topic of this query.But things are little bit different here. Because this query classifier is a subproject for collaborative search engine.&lt;/p&gt;

&lt;h2 id=&#34;collaborative-search-engine&#34;&gt;Collaborative Search Engine&lt;/h2&gt;

&lt;p&gt;That is a search engine platform for a team not an individual (As you know, most of current search engine are just personized for individual). In here we set some experiments according to the real situation that when a team is addressing some problems around a certain task. For example, we assumed a team are planning to go for a travel, so one of guy focus on booking hotel or airline ticket, one guy focus on studying the route of attractions. Then the collaborative search engine should give each person different ranking results.&lt;/p&gt;

&lt;h2 id=&#34;why-need-query-classifier&#34;&gt;Why need query classifier?&lt;/h2&gt;

&lt;p&gt;We assume that during the collaborative search engine working, a team is working on a certain task. And there is a task statement wrote by natural language. In task statement, the subtopic also indicated by some sentences. (Experiment initial stage) So my query classifier is trying to figure out the input query belong to what kind of subtopic under this task.&lt;/p&gt;

&lt;h2 id=&#34;what-does-classifier-result-look-like&#34;&gt;What does classifier result look like?&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ve set two experimental tasks for team searching. One is (Study on Social Media). Another is travel in Helsinki. I did some example queries on Console and screenshot results.&lt;/p&gt;

&lt;p&gt;For traveling task, we assumed we have three people searching on three aspects in Helsinki: culture, dining and outdoor activity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://xmxiaohuilin.github.io/media/TravelTask.png&#34; alt=&#34;&#34; &gt;&lt;/p&gt;

&lt;p&gt;For social media study, we assumed five subtopics for team member:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Emergence and spread of social networking sites, such as MySpace, Facebook, Twitter, and del.icio.us&lt;/li&gt;
&lt;li&gt;Statistics about popularity of such sites-(How many users? How much time they spend? How much content?)&lt;/li&gt;
&lt;li&gt;Impacts on students and professionals&lt;/li&gt;
&lt;li&gt;Commerce around these sites-(How do they make money? How do users use them to make money?)&lt;/li&gt;
&lt;li&gt;Examples of usage of such services in various domains, such as healthcare and politics&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://xmxiaohuilin.github.io/media/Socialmedia1.png&#34; alt=&#34;&#34; &gt;
&lt;img src=&#34;http://xmxiaohuilin.github.io/media/Socialmedia2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;

&lt;p&gt;How about them by scores and rank?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://xmxiaohuilin.github.io/media/Socialmedia3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;

&lt;h2 id=&#34;procedure&#34;&gt;Procedure:&lt;/h2&gt;

&lt;h4 id=&#34;step-0-basic-idea&#34;&gt;Step 0. Basic idea&lt;/h4&gt;

&lt;p&gt;Build language model for each subtopic in one task and get the relevance score between query and subtopic model.&lt;/p&gt;

&lt;h4 id=&#34;step-1-corpus&#34;&gt;Step 1. Corpus&lt;/h4&gt;

&lt;p&gt;Set up Corpus for task and each subtopic
 - Keywords extraction from task statement
     - We believe the keyword should be noun or proper noun or noun phrase from task statement.
     - Then Stanford NLP parser to get part-of-speech tags.&lt;br /&gt;
     - There is a little bit tricky during finding the noun phrase. As we know the NLP parser generate the tagging sentences as a tree. Every leaf of tree is the word in sentence. But we need look up the parent nodes or grandparent nodes of leaves to get the phrase tag. So each time even if we find a word is noun or proper noun we still need to check its upper level nodes and consider its tags.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Build keywords as query on Google

&lt;ul&gt;
&lt;li&gt;A subtopic can be represent by some keywords combination:&lt;/li&gt;
&lt;li&gt;Those keywords can be formed as the queries&lt;/li&gt;
&lt;li&gt;Fetch the Google Top 20 Results title and snippet as the subtopic model&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;step-2-evaluate-query-and-subtopic-relevance&#34;&gt;Step 2. Evaluate Query and Subtopic Relevance&lt;/h4&gt;

&lt;p&gt;I consider both classical statistic model and language model and get relevance score from two part, but different weights on two models (0.8 and 0.2). As we know language model has better performance. Because, the language model is more rely on the real language rules other than the mathematical statistical rules and  the details of how statistics like term frequency and document
length are used differ.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Query Likelihood Model: Dirichlet Smoothing Algorithm&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;considering term frequency and collection frequency&lt;/li&gt;
&lt;li&gt;Using a reference model (collection language model) to discriminate unseen words.
    $$P(w|D) = \frac{c(w,M_D)+\mu\cdot P(w|M_C)}{|D|+\mu}$$
    |D| means the length of current document!
    $c(w,M_D)$ means the term frequency in a document&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Vector Space Model: using TF-IDF score&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The vector between subtopic model and query&lt;/li&gt;
&lt;li&gt;but did some query expansion: top 5 snippet from google result as expansion&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;how-to-implement&#34;&gt;How to implement?&lt;/h2&gt;

&lt;h3 id=&#34;subtopic-language-model&#34;&gt;Subtopic Language Model&lt;/h3&gt;

&lt;h4 id=&#34;1-keyword-extraction&#34;&gt;1. Keyword Extraction&lt;/h4&gt;

&lt;p&gt;Keywords extraction means to extracted keywords from task statement and subtopic statement. These keywords would be combined as queries to search on Google and crawl their Top 20 result as the language model for subtopic.
In this study, I use the Stanford Nature language parser package to extraction noun word leaves from statement as a keyword, moreover, if a certain noun word leaf has parent node which is noun phrase, the noun phrase should be used as keyword.&lt;/p&gt;

&lt;p&gt;Keyword Extraction Example from Task 2 and Task 5
(Word with “+” means the proper noun)&lt;/p&gt;

&lt;p&gt;Keywords for Task 2:
Emergence, spread, social networking sites, Facebook+, Myspace+, Twitter+, Delicious+, statistics, popularity, sites, users, time, impacts, students, professionals, commerce, money, examples, usage, services, domains, healthcare, politics&lt;/p&gt;

&lt;p&gt;Keywords for Task 5:
 friend, four-day vacation, December+, Helsinki+, Finland+, information, vacation, flights, US+, hotels, activities, goal, joint plan, things, Euros, person, group, vacation, outdoor activity, dining activity, cultural activity, types, addition&lt;/p&gt;

&lt;h4 id=&#34;2-corpus-set-up&#34;&gt;2. Corpus Set Up&lt;/h4&gt;

&lt;p&gt;According to the keywords extracted from task or subtopic statement, these keywords can be combined as query for each subtopic. Here are the combination rules:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1)   Keywords from task statement should regarded as a collection.&lt;/li&gt;
&lt;li&gt;2)   Proper Noun words should regarded as a collection.&lt;/li&gt;
&lt;li&gt;3)   Keywords from each subtopic statement should regarded as a collection.&lt;/li&gt;
&lt;li&gt;4)   Those keywords come from three collections combined as a query represent for each subtopic.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using these query and search the TOP 20 results on Google, the titles and snippets on the Google result pages can regarded as the corpus for describing each subtopic .&lt;/p&gt;

&lt;h4 id=&#34;3-dirichlet-prior-smoothing&#34;&gt;3. Dirichlet Prior Smoothing&lt;/h4&gt;

&lt;p&gt;Once the corpus built up, We nned build language model for getting the likehood of each query belong to what kind of subtopic. I use Dirichlet Prior Smoothing (DPS) method to get the probability of each term from a query in a certain subtopic language model. Then get product by these probability. The probability can be regarded as score one.&lt;/p&gt;

&lt;h4 id=&#34;4-collection-expansion&#34;&gt;4. Collection Expansion&lt;/h4&gt;

&lt;p&gt;However, sometimes the term may not get any frequency in collection which is shown as c(w,D) in equation due to the collection may not rich enough for some terms. To deal with this problem, I tried to use the title of task as query to search on Google and get its TOP 50 result as the collection background.&lt;/p&gt;

&lt;h2 id=&#34;query-expansion&#34;&gt;Query Expansion&lt;/h2&gt;

&lt;h4 id=&#34;1-expansion-content&#34;&gt;1. Expansion Content&lt;/h4&gt;

&lt;p&gt;For each user query, I first search the query on Google with fetching its TOP 5 results. The titles and snippets are combined as the query’s expansion.&lt;/p&gt;

&lt;h4 id=&#34;2-tfidf-similarity-by-vsm&#34;&gt;2. TFIDF Similarity by VSM&lt;/h4&gt;

&lt;p&gt;For each query expansion content and subtopic description content (Language model), they can be figured out with two vectors according the terms TFIDF values in two contents. Using the VSM model with these two vectors, then can calculate the similarity between query expansion content and subtopic description. The similarity value can be regarded as score two in this study.&lt;/p&gt;

&lt;h3 id=&#34;3-performance-progress&#34;&gt;3.  Performance Progress&lt;/h3&gt;

&lt;p&gt;The experiments are stepping by several stages. At the first stage, only language model with Dirichlet Prior Smoothing score applied to evaluate whether a query matching with the subtopic. Under this case, the precision  of experiment is just over than 0.64. Then after inserting the collection background, the precision is improved to 0.73. Finally, with combining DFS score and VSM similarity score, the precision has lift to over 0.81, which is acceptable for the study. So far, we decide to leave the rest of possible features and only use the DFS score and VSM similarity score for the query – subtopic rank system.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>