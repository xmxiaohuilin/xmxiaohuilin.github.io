<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mongodb on Cindy&#39;s Blog</title>
    <link>http://xmxiaohuilin.github.io/tags/mongodb/</link>
    <description>Recent content in Mongodb on Cindy&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>(c) 2015 Cindy Lin.</copyright>
    <lastBuildDate>Thu, 16 Apr 2015 12:56:15 -0700</lastBuildDate>
    <atom:link href="http://xmxiaohuilin.github.io/tags/mongodb/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>How to Input and Output with Mongo DB Data on Hadoop Platform</title>
      <link>http://xmxiaohuilin.github.io/code/MongoDBwithHadoop/</link>
      <pubDate>Thu, 16 Apr 2015 12:56:15 -0700</pubDate>
      
      <guid>http://xmxiaohuilin.github.io/code/MongoDBwithHadoop/</guid>
      <description>

&lt;p&gt;In the previous article, I&amp;rsquo;ve talked about how to distributed a Mongo DB database in multiple virtual machine. Here, I&amp;rsquo;d like to discuss how we can make data stream between Mongo DB and Hadoop. In fact, the format from MongoDB cannot directly used in Hadoop Map Reduce function. You have to use Mongo Hadoop API to assist you for finishing this difficult connection between MongoDB and Hadoop.&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The MongoDB Connector for Hadoop is a plugin for Hadoop that provides the ability to use MongoDB as an input source and/or an output destination. The major advantage is that once the connector is implemented, the analytic power of Hadoop can be utilized with the MongoDB storage architecture.
&amp;gt; “The Connector presents MongoDB as a Hadoop-compatible file system allowing a MapReduce job to read from MongoDB directly without first copying it to HDFS, thereby eliminating the need to move Terabytes of data across the network. MapReduce jobs can pass queries as filters, so avoiding the need to scan entire collections, and can also take advantage of MongoDB’s rich indexing capabilities including geospatial, text-search, array, compound and sparse indexes”&lt;/p&gt;

&lt;h2 id=&#34;procedure&#34;&gt;Procedure&lt;/h2&gt;

&lt;p&gt;Once we choose MongoDB as the I/O target, all of our I/O format needs to fit with MongoDB document format, thus we must use either JSON or BSON. So the entire procedure when Hadoop works with MongoDB is shown as follows:
- Create MongoDB URL builder with multiple Mongos (Routers) as Database entrances
- Set one MongoDB collection as input source with MongoDB URL builder
- Use Hadoop as Map-Reduce computing framework.
- Override Mapper class with BSON format as value in.
- Override Reduce class addressing the BSON data and output the data with MongodbUpdateWritable format
- Set another MongoDB collection as output destination  with MongoDB URL builder&lt;/p&gt;

&lt;h2 id=&#34;talk-is-cheap-show-me-the-code&#34;&gt;Talk is cheap, show me the code!&lt;/h2&gt;

&lt;h3 id=&#34;access-to-mongo-db&#34;&gt;Access to Mongo DB&lt;/h3&gt;

&lt;p&gt;By create MongoDB URL builder, the hadoop platform can detect the database position. However, we have multiple Mongo DB routers as Database entrances. So we just save our entry IP address as a list.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;private MongoClientURIBuilder ShardedDBURIBuilder() {
		// mainly host
		String mainHost = &amp;quot;44XX.XXX.XXX.X01&amp;quot;;

		// set up the option entrances when the main one shut down
		List&amp;lt;ServerAddress&amp;gt; serverSeeds;
		MongoClient mongoClient = null;
		serverSeeds = new ArrayList&amp;lt;ServerAddress&amp;gt;();
		try {
			serverSeeds.add(new ServerAddress(&amp;quot;4XX.XXX.XXX.X02&amp;quot;, 27017));
			serverSeeds.add(new ServerAddress(&amp;quot;4XX.XXX.XXX.X03&amp;quot;, 27017));
			mongoClient = new MongoClient(serverSeeds);
			mongoClient.getMongoClientOptions();
		} catch (UnknownHostException e) {
			log.error(e + &amp;quot;&amp;quot; + e.getCause());
		}

		MongoClientURIBuilder uriBuilder = new MongoClientURIBuilder();
		uriBuilder.addHost(mainHost, 27017);
		uriBuilder.options(mongoClient.getMongoClientOptions());
		return uriBuilder;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we know where is the Mongo DB! But we still need to locate the collection in MongoDB and more detailed information. Here we go:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;MongoClientURIBuilder uriBuilder = ShardedDBURIBuilder();
uriBuilder.collection(&amp;quot;stock&amp;quot;, &amp;quot;symbols&amp;quot;);
MongoClientURI inputURI = uriBuilder.build();
MongoConfigUtil.setInputURI(getConf(), inputURI);
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;build-our-own-mapper-and-reducer&#34;&gt;Build our own mapper and reducer!&lt;/h3&gt;

&lt;p&gt;Here I use my project about stock information crawler as an example. Mapper read the stock symbol data from the MongoDB. Then it get a list of symbol name in mapper. The mapper has separated that list and the reducer should read different part of list and search the stock information according to the partial symbol name list.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Override Mapper class with BSON format as value in.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public class SymbolsMapper extends Mapper&amp;lt;Object, BSONObject, Text, IntWritable&amp;gt;{
    @Override
    public void map(Object key, BSONObject val, final Context context) 
        throws IOException, InterruptedException {
    	System.out.println(val.get(&amp;quot;symbol&amp;quot;)+&amp;quot;  Mapper Getted!!&amp;quot;);
        context.write(new Text((val.get(&amp;quot;symbol&amp;quot;)).toString()), 
        		new IntWritable(1));
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Override Reduce class addressing the BSON data and output the data with MongodbUpdateWritable format&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public class SymbolsReducer extends Reducer&amp;lt;Text, IntWritable, NullWritable, MongoUpdateWritable&amp;gt;{
	
    @Override
    public void reduce(final Text pKey, final Iterable&amp;lt;IntWritable&amp;gt; pValues,
                        final Context pContext )
            throws IOException, InterruptedException{
    	    
        StockCrawler stockCrawler = new StockCrawler();
        
        // get symbol from keyIn 
        Quote quote = stockCrawler.getHistQuotesBySymbol(pKey.toString());
        if(quote==null||quote.getSymbolName()==null||quote.getHistorical_quotes()==null)
        	return;
        // get Quote info, set new id
        BasicBSONObject query = new BasicBSONObject(&amp;quot;_id&amp;quot;, quote.getKey());
       
        // set symbol name and symbol quotes
        BasicBSONObject stockQuote = new BasicBSONObject();
        stockQuote.put(&amp;quot;symbol&amp;quot;, quote.getSymbolName());
        ArrayList&amp;lt;Object&amp;gt; historical_quotes =  quote.getHistorical_quotes();
        
        BasicBSONObject update = new BasicBSONObject(&amp;quot;$set&amp;quot;, stockQuote);
        update.append(&amp;quot;$pushAll&amp;quot;, new BasicBSONObject(&amp;quot;historical_quotes&amp;quot;, historical_quotes));
        
        pContext.write(null, new MongoUpdateWritable(query, update, true, false));
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;output-back-to-mongo-db&#34;&gt;Output back to Mongo DB&lt;/h3&gt;

&lt;p&gt;The output procedure is very simliar with the input one. But just the difference on the target collection in Mongo DB.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;MongoClientURI outputURI = uriBuilder.build();
MongoConfigUtil.setOutputURI(getConf(), outputURI);
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;the-overview-on-this-connector&#34;&gt;The overview on this connector:&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public MongoMapredStockCrawler() {
		setConf(new Configuration());

		if (MongoTool.isMapRedV1()) {
			MapredMongoConfigUtil.setInputFormat(getConf(),
					com.mongodb.hadoop.mapred.MongoInputFormat.class);
			MapredMongoConfigUtil.setOutputFormat(getConf(),
					com.mongodb.hadoop.mapred.MongoOutputFormat.class);
		} else {
			MongoConfigUtil.setInputFormat(getConf(), MongoInputFormat.class);
			MongoConfigUtil.setOutputFormat(getConf(), MongoOutputFormat.class);
		}

		MongoClientURIBuilder uriBuilder = ShardedDBURIBuilder();
		uriBuilder.collection(&amp;quot;stock&amp;quot;, &amp;quot;symbols&amp;quot;);
		MongoClientURI inputURI = uriBuilder.build();
		uriBuilder.collection(&amp;quot;stock&amp;quot;, &amp;quot;quotes&amp;quot;);
		MongoClientURI outputURI = uriBuilder.build();

		MongoConfigUtil.setInputURI(getConf(), inputURI);
		MongoConfigUtil.setOutputURI(getConf(), outputURI);

		MongoConfigUtil.setMapper(getConf(), SymbolsMapper.class);
		MongoConfigUtil.setReducer(getConf(), SymbolsReducer.class);

		MongoConfigUtil.setMapperOutputKey(getConf(), Text.class);
		MongoConfigUtil.setMapperOutputValue(getConf(), IntWritable.class);

		MongoConfigUtil.setOutputKey(getConf(), IntWritable.class);
		MongoConfigUtil.setOutputValue(getConf(), BSONWritable.class);
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This is very typical example when people need to read data from Mongo DB and process the data on Hadoop platform and then back the data to Mongo DB. Since there is very little instruction about this work, I just shared my idea on that.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Distributed MongoDB Configuration</title>
      <link>http://xmxiaohuilin.github.io/code/DistributedMongoDB/</link>
      <pubDate>Tue, 10 Mar 2015 22:56:15 -0700</pubDate>
      
      <guid>http://xmxiaohuilin.github.io/code/DistributedMongoDB/</guid>
      <description>

&lt;p&gt;How to shard the distributed Mongo DB in remote VMs? Here is what I did during a project using Mongo DB as Database and using Hadoop as computing framework.&lt;/p&gt;

&lt;h2 id=&#34;sharded-mongodb-configuration&#34;&gt;Sharded MongoDB Configuration&lt;/h2&gt;

&lt;p&gt;The following graph is the architecture of how I set three VMs with different port to simulate the real sharding pattern(which need 15 machines actually)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://xmxiaohuilin.github.io/media/Sharded%20MongoDB.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;

&lt;p&gt;The following procedure is how I configured MongoDB on remote three VMs.&lt;/p&gt;

&lt;h3 id=&#34;1-set-up-data-path-config-file-and-log-file-paths-in-each-node-with-mongos-config-shard1-shard2-shard3-directory-name&#34;&gt;1. Set up data path, config file and log file paths in each node with mongos 、config 、 shard1 、shard2、shard3 (directory name)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /data/mongos/log
sudo chmod -R 777 /data/mongos/log
mkdir -p /data/config/data
sudo chmod -R 777 /data/config/data
mkdir -p /data/config/log
sudo chmod -R 777 /data/config/log
mkdir -p /data/mongos/log
sudo chmod -R 777 /data/mongos/log
mkdir -p /data/shard1/data
sudo chmod -R 777 /data/shard1/data
mkdir -p /data/shard1/log
sudo chmod -R 777 /data/shard1/log
mkdir -p /data/shard2/data
sudo chmod -R 777 /data/shard2/data
mkdir -p /data/shard2/log
sudo chmod -R 777 /data/shard2/log
mkdir -p /data/shard3/data
sudo chmod -R 777 /data/shard3/data
mkdir -p /data/shard3/log
sudo chmod -R 777 /data/shard3/log
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-make-a-plan-of-port-number-and-modify-some-config-parameter-in-mongod-conf&#34;&gt;2. Make a plan of port number and modify some config parameter in mongod.conf&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;mongod --configsvr --dbpath /data/config/data --port 27019 --logpath /data/config/log/config.log --fork


mongos  --configdb 45.55.188.234:27019,45.55.186.238:27019,104.131.106.22:27019  --port 27017   --logpath  /data/mongos/log/mongos.log --fork

## or 
## These codes can migration configuration 

rsync -az /data/configdb mongo-config1.example.net:/data/configdb
rsync -az /data/configdb mongo-config2.example.net:/data/configdb

nano etc/mongod.conf

### This is important!!
set bing_ip = 0.0.0.0; for remote login
set default mongod: 27019 
## !Otherwise your mongos port(27017) will be blocked
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-config-sharding-setting-on-each-vm&#34;&gt;3. Config sharding setting on each VM&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;### set up shards ports and dbpath and log path
mongod --shardsvr --replSet shard1 --port 22001 --dbpath /data/shard1/data  --logpath /data/shard1/log/shard1.log --fork --journal  --oplogSize 10
mongod --shardsvr --replSet shard2 --port 22002 --dbpath /data/shard2/data  --logpath /data/shard2/log/shard2.log --fork --journal  --oplogSize 10
mongod --shardsvr --replSet shard3 --port 22003 --dbpath /data/shard3/data  --logpath /data/shard3/log/shard3.log --fork --journal  --oplogSize 10


# Shard_1 in Node(45.55.188.234)
mongo  127.0.0.1:22001
use admin
config = { _id:&amp;quot;shard1&amp;quot;, members:[
                     {_id:0,host:&amp;quot;45.55.188.234:22001&amp;quot;},
                     {_id:1,host:&amp;quot;45.55.186.238:22001&amp;quot;},
                {_id:2,host:&amp;quot;104.131.106.22:22001&amp;quot;,arbiterOnly:true}
                ]
         }
rs.initiate(config);

# Shard_2 in Node(45.55.186.238)
mongo  127.0.0.1:22002
use admin
config = { _id:&amp;quot;shard2&amp;quot;, members:[
                     {_id:0,host:&amp;quot;45.55.186.238:22002&amp;quot;},
                     {_id:1,host:&amp;quot;104.131.106.22:22002&amp;quot;},
                {_id:2,host:&amp;quot;45.55.188.234:22002&amp;quot;,arbiterOnly:true}
                ]
         }
rs.initiate(config);

# Shard_3 in Node(104.131.106.22)
mongo  127.0.0.1:22003
use admin
config = { _id:&amp;quot;shard3&amp;quot;, members:[
                     {_id:0,host:&amp;quot;104.131.106.22:22003&amp;quot;},
                     {_id:1,host:&amp;quot;45.55.188.234:22003&amp;quot;},
           {_id:2,host:&amp;quot;45.55.186.238:22003&amp;quot;,arbiterOnly:true}
                ]
         }
rs.initiate(config);
## if you need to reconfig, please use Cmd( rs.reconfig(your_para) )
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-add-shard-config-just-in-one-of-vms&#34;&gt;4. Add Shard Config just in one of VMs&lt;/h3&gt;

&lt;p&gt;It seems no master mode concept in MongoDB. So just choose one of it. Config Sharding info in mongos; I also made the sharding part on different machines (e.g. Primary Shard1 on Server One, Primary Shard2 on Server Two).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## no specific addr/port 
mongo
use admin

db.runCommand({addshard : &amp;quot;shard1/45.55.188.234:22001,45.55.186.238:22001,104.131.106.22:22001&amp;quot;});

db.runCommand({addshard: &amp;quot;shard2/45.55.186.238:22002,104.131.106.22:22002,45.55.188.234:22002&amp;quot;});

db.runCommand({addshard : &amp;quot;shard3/104.131.106.22:22003,45.55.188.234:22003,45.55.186.238:22003&amp;quot;});

## if you need to reset your previous setting
## 
db.runCommand( { removeShard: &amp;quot;shard1&amp;quot; } )

#Test:
db.runCommand( { enablesharding :&amp;quot;stock&amp;quot;});
db.runCommand( { shardcollection : &amp;quot;stock.quotes&amp;quot;,key : {&amp;quot;_id&amp;quot;: 1} })

for (var i = 1; i &amp;lt;= 100000; i++) db.table1.save({id:i,&amp;quot;test1&amp;quot;:&amp;quot;testval1&amp;quot;});


use admin
db.addUser(&#39;test&#39;,&#39;test&#39;)
db.auth(&#39;test&#39;,&#39;test&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-some-commmands-for-check-database-status&#34;&gt;5. Some Commmands for check Database Status;&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;db.stats();

show databases

db.dropDatabase()

db.printShardingStatus()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;6-misc&#34;&gt;6. Misc.&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;## Sometimes export jar failed
 zip -d stockCrawler.jar META-INF/LICENSE
 jar tvf stockCrawler.jar | grep -i license
 
 ## HDFS manipulation
 
 hadoop fs -ls
 hadoop fs -mkdir /user/${adminName}   
 hadoop fs -touch test
 hdfs dfs -copyFromLocal ${fileName}
 hdfs dfs -cat ${fileName}
 hadoop fs -rmr output
 hadoop jar stockCrawler.jar
 
 ## Some query example:
 db.quotes.find({&#39;historical_quotes.date&#39;:&#39;2015-04-10&#39;})
 
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>